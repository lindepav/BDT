{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr/Rk9B3Q0uNH0BmHL1nMo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let's have some fun with code "
      ],
      "metadata": {
        "id": "Dj8aXelUgZdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to connect to Kafka's broker. The broker is hosted as[ MSK Kafka AWS Service](https://aws.amazon.com/msk/).\n",
        "\n",
        "\n",
        "\n",
        "1.   broker1 b-1-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196\n",
        "2.   broker2  b-2-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196\n",
        "\n",
        "You will need user name and password, provided on previous lectures. \n"
      ],
      "metadata": {
        "id": "o3mDGB-lgdDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to broker\n",
        "JAAS = 'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"USERXX\" password=\"PWDXX\";'\n",
        "tram_stream_topic = spark.readStream \\\n",
        "  .format(\"kafka\")\\\n",
        "  .option(\"kafka.bootstrap.servers\", \"b-2-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196, b-1-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196\") \\\n",
        "  .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-512\")\\\n",
        "  .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
        "  .option(\"kafka.sasl.jaas.config\", JAAS) \\\n",
        "  .option(\"subscribe\", \"trams\") \\\n",
        "  .load()"
      ],
      "metadata": {
        "id": "XUhSQTsZhSw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to cast json messages directly,  using schema, or you can cast it just to string, but it might complicate your work later. \n",
        "\n",
        "Use schema below. It is possible to save it to other python or dbx notebook and call it externally, see example below for dbx notebook.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "%run \"./pid_schema\" # the notebook's name with function in it\n",
        "schema_pid = get_pid_schema() # use it for casting later\n",
        "```\n",
        "\n",
        "The schema must be wrapped into function with return."
      ],
      "metadata": {
        "id": "x1RhAZt8l99n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "  schema_pid = StructType([\n",
        "      StructField('geometry', StructType([\n",
        "          StructField('coordinates', ArrayType(StringType()), True),\n",
        "          StructField('type', StringType())])),\n",
        "      StructField('properties', StructType([\n",
        "          StructField('last_position', StructType([\n",
        "              StructField('bearing', IntegerType()),\n",
        "              StructField('delay', StructType([\n",
        "                  StructField(\"actual\", IntegerType()),\n",
        "                  StructField(\"last_stop_arrival\", StringType()),\n",
        "                  StructField(\"last_stop_departure\", StringType())])),\n",
        "              StructField(\"is_canceled\", BooleanType()),\n",
        "              StructField('last_stop', StructType([\n",
        "                  StructField(\"arrival_time\", StringType()),\n",
        "                  StructField(\"departure_time\", StringType()),\n",
        "                  StructField(\"id\", StringType()),\n",
        "                  StructField(\"sequence\", IntegerType())])),\n",
        "              StructField('next_stop', StructType([\n",
        "                  StructField(\"arrival_time\", StringType()),\n",
        "                  StructField(\"departure_time\", StringType()),\n",
        "                  StructField(\"id\", StringType()),\n",
        "                  StructField(\"sequence\", IntegerType())])),\n",
        "              StructField(\"origin_timestamp\", StringType()),\n",
        "              StructField(\"shape_dist_traveled\", StringType()),\n",
        "              StructField(\"speed\", StringType()),\n",
        "              StructField(\"state_position\", StringType()),\n",
        "              StructField(\"tracking\", BooleanType())])),\n",
        "          StructField('trip', StructType([\n",
        "              StructField('agency_name', StructType([\n",
        "                  StructField(\"real\", StringType()),\n",
        "                  StructField(\"scheduled\", StringType())])),\n",
        "              StructField('cis', StructType([\n",
        "                  StructField(\"line_id\", StringType()),\n",
        "                  StructField(\"trip_number\", StringType())])),\n",
        "              StructField('gtfs', StructType([\n",
        "                  StructField(\"route_id\", StringType()),\n",
        "                  StructField(\"route_short_name\", StringType()),\n",
        "                  StructField(\"route_type\", IntegerType()),\n",
        "                  StructField(\"trip_headsign\", StringType()),\n",
        "                  StructField(\"trip_id\", StringType()),\n",
        "                  StructField(\"trip_short_name\", StringType())])),\n",
        "              StructField(\"origin_route_name\", StringType()),\n",
        "              StructField(\"sequence_id\", IntegerType()),\n",
        "              StructField(\"start_timestamp\", StringType()),\n",
        "              StructField(\"vehicle_registration_number\", IntegerType()),\n",
        "              StructField('vehicle_type', StructType([\n",
        "                  StructField(\"description_cs\", StringType()),\n",
        "                  StructField(\"description_en\", StringType()),\n",
        "                  StructField(\"id\", IntegerType())])),\n",
        "              StructField(\"wheelchair_accessible\", BooleanType()),\n",
        "              StructField(\"air_conditioned\", BooleanType())]))])),\n",
        "      StructField(\"type\", StringType())\n",
        "  ])"
      ],
      "metadata": {
        "id": "gLQd9UeImVKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import from_json, col\n",
        "base_trams = tram_stream_topic.select(from_json(col(\"value\").cast(\"string\"), schema_pid).alias(\"data\")).select(\"data.*\") \\"
      ],
      "metadata": {
        "id": "D3IQ48TRlKrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start the actual spark stream. There are sevrral ways, how to store data, which output mode etc. //TODO links\n",
        "\n",
        "Now we use format memory - data will be stored in memory and we will append, ie. we do not wait for complete data (in specified batch or so).\n",
        "\n"
      ],
      "metadata": {
        "id": "ds_1fRUYn8Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "tram_stream_mem_append = base_trams.writeStream \\\n",
        "        .format(\"memory\")\\\n",
        "        .queryName(\"mem_trams\")\\\n",
        "        .outputMode(\"append\")\\\n",
        "        .start()"
      ],
      "metadata": {
        "id": "wvMXS96QoMAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File saving?"
      ],
      "metadata": {
        "id": "CeX5mQJepOAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "tram_stream_file = base_trams.writeStream \\\n",
        "        .format(\"memory\")\\\n",
        "        .queryName(\"mem_trams\")\\\n",
        "        .outputMode(\"append\")\\\n",
        "        .start()"
      ],
      "metadata": {
        "id": "DHg9TTSwpQg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "qOV9kYq8iMDu"
      }
    }
  ]
}