{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opakování &ndash; užitečné věci z Pythonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delka stringu je: 18\n",
      "String obsahuje slova: ['to', 'be', 'Or', 'NOT', 'to', 'be']\n",
      "String obsahuje unikatni slova: {'be', 'NOT', 'Or', 'to'}\n",
      "String prevedeny na mala pismena:  to be or not to be\n",
      "Delka pole (pocet slov) je: 6\n",
      "Prvni prvek v poli je: to\n",
      "Posledni prvek v poli je: be\n"
     ]
    }
   ],
   "source": [
    "my_string = \"to be Or NOT to be\"\n",
    "my_list = my_string.split()\n",
    "print ('Delka stringu je:', len(my_string))\n",
    "print ('String obsahuje slova:', my_string.split())\n",
    "print ('String obsahuje unikatni slova:', set(my_string.split()))\n",
    "print ('String prevedeny na mala pismena: ', my_string.lower())\n",
    "print ('Delka pole (pocet slov) je:', len(my_list))\n",
    "print ('Prvni prvek v poli je:', my_list[0])\n",
    "print ('Posledni prvek v poli je:', my_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spuštění interaktivního shellu PySpark\n",
    "\n",
    "`export PYSPARK_PYTHON=python3`  \n",
    "`pyspark --master yarn --num-executors 2 --executor-memory 4G --conf spark.ui.port=1<ddmm>`, kde `<ddmm>` je váš den a měsíc narození, např. `spark.ui.port=10811`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Úkol č. 1: rozběhání příkladu z přednášky (word count)\n",
    "\n",
    "Vyzkoušejte, že příklad z přednášky funguje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nacteni RDD\n",
    "lines = sc.textFile(\"/user/pascepet/data/bible/bible.txt\")\n",
    "\n",
    "# rozdeleni radku\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# transformace radku na (key, value)\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# secteni jednicek ke kazdemu klici\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# vypis vysledku\n",
    "counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Úkol č. 2: vylepšení příkladu z přednášky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vylepšete skript z úkolu č. 1 o následující funkcionality:\n",
    "\n",
    "2.1. Ignorujte začátky řádků (název biblické knihy a kód kapitola:verš &ndash; jsou odděleny od textu tabulátorem).  \n",
    "2.2. Slova, která se liší jen velikostí písmen, se budou považovat za stejná.  \n",
    "2.3. Odstraňte z textu nealfanumerické znaky &ndash; všechny nebo aspoň některé z nich (např. '.', ':', '-' atd.).  \n",
    "2.4. Vyřaďte ze zpracování slova v množině tzv. *stopwords* (v souboru `/user/pascepet/data/stopwords.txt`).   \n",
    "*Hint: stopwords můžete dostat do proměnné typu set např. takto:*  \n",
    "`sw = set(sc.textFile('/user/pascepet/data/stopwords.txt').collect())`  \n",
    "2.5. Vyberte RDD, které je vhodné si pro další výpočty kešovat, a nakešujte ho.  \n",
    "2.6. Na konci seřaďte slova podle četnosti sestupně (použijte [metodu sortBy](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.sortBy)).  \n",
    "2.7. Najděte nejdelší slovo (s největším počtem znaků)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package re kvuli pouziti regularniho vyrazu\n",
    "import re\n",
    "\n",
    "# nacteni RDD\n",
    "lines = sc.textFile(\"/user/pascepet/data/bible/bible.txt\")\n",
    "\n",
    "# vzit cast radku po tabulatoru (zacatek radku se zahodi)\n",
    "lines2 = lines.map(lambda line: line.split(\"\\t\")[1])\n",
    "\n",
    "# rozdeleni radku\n",
    "words = lines2.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# prevedeni slov na mala pismena\n",
    "# vypusteni nealfanumerickych znaku\n",
    "# ponechani jen slov, ktera nejsou stopwords\n",
    "sw = set(sc.textFile('/user/pascepet/data/stopwords.txt').collect())\n",
    "words2 = words.map(lambda word: word.lower()) \\\n",
    "    .map(lambda word: re.sub(r'\\W+', '', word)) \\\n",
    "    .filter(lambda word: word not in sw)\n",
    "\n",
    "# poznamka: odstraneni jen urcitych znaku lze provest napr. takto\n",
    "# words2 = words2.map(lambda word: word.replace('.', '')\n",
    "\n",
    "# nakesovani\n",
    "words2.cache()\n",
    "# poznamka: v pripade pouziti metody persist je treba importovat StorageLevel\n",
    "# from pyspark import StorageLevel\n",
    "# words2.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# transformace radku na (key, value)\n",
    "pairs = words2.map(lambda word: (word, 1))\n",
    "\n",
    "# secteni jednicek ke kazdemu klici\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# serazeni podle cetnosti\n",
    "counts_sorted = counts.sortBy(lambda item: item[1], ascending=False)\n",
    "\n",
    "# vypis vysledku\n",
    "counts_sorted.take(20)\n",
    "\n",
    "# vypocet nejdelsiho slova - prirazeni delky ke slovu\n",
    "words_len = words2.distinct() \\\n",
    "    .map(lambda word: (word, len(word)))\n",
    "\n",
    "# setrideni slov podle delky\n",
    "words_len_sorted = words_len.sortBy(lambda w: w[1], ascending=False)\n",
    "words_len_sorted.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Úkol č. 3: další analýza textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V této části budeme pracovat s původním textem.\n",
    "\n",
    "3.1. Zjistěte počet slov v každém verši (1 verš = 1 řádek) a najděte verše s největšim a nejmenším počtem slov.  \n",
    "3.2. Proveďte stejný výpočet jako v 3.1, ale v každém verši počítejte jen unikátní slova (každé slovo jen jednou)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nejdelsi a nejkratsi vers\n",
    "# oddeli se zacatek radku a zbytek\n",
    "line_split = lines.map(lambda line: line.split(\"\\t\"))\n",
    "\n",
    "# ze zbytku radku se zjisti pocet slov a radky se podle toho seradi\n",
    "lines_len = line_split.map(lambda line: (line[0], len(line[1].split(\" \")))) \\\n",
    "    .sortBy(lambda vers: vers[1], ascending=False)\n",
    "\n",
    "lines_len.take(1)\n",
    "lines_len.sortBy(lambda vers: vers[1], ascending=True).take(1)\n",
    "\n",
    "# nejdelsi a nejkratsi vers s unikatnimi slovy\n",
    "# tentokrat se ze zbytku radku nejdriv vezmou jen unikatni slova a az ta se spocitaji\n",
    "lines_len_uniq = line_split.map(lambda line: (line[0], len(set(line[1].split(\" \"))))) \\\n",
    "    .sortBy(lambda vers: vers[1], ascending=False)\n",
    "\n",
    "lines_len_uniq.take(1)\n",
    "lines_len_uniq.sortBy(lambda vers: vers[1], ascending=True).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Úkol č. 4: číselné výpočty z dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Budeme pracovat se soubory hodinových údajů o teplotách (použili jsme ho na supercvičení Hive). Pokud jste na tomto supercvičení byli, pravděpodobně už máte data na HDFS (v podadresáři svého pracovního adresáře, jsou to soubory `teplota1-6.csv` a `teplota7-12.csv`) Zkontrolujte to.  \n",
    "Nemáte-li je tam, proveďte následující kroky:\n",
    "\n",
    "* zkopírujte soubor `/home/pascepet/fel_bigdata/data/teplota-usa.zip` do svého uživatelského adresáře (doporučuji si na to vytvořit podadresář);\n",
    "* rozbalte;\n",
    "* vytvořte na HDFS ve svém uživatelském adresáři vhodný podadresář;\n",
    "* rozbalené soubory nakopírujte na HDFS do nově vytvořeného adresáře.\n",
    "\n",
    "CSV soubor má jako oddělovač znak `','`. Také obsahuje hlavičky s názvy sloupců, které je potřeba při zpracování odstranit.    \n",
    "Teplota je uvedena v 10&times;&deg;F. Některé řádky obsahují na místě teploty prázdný řetězec, tj. údaj pro ně není k dispozici.\n",
    "\n",
    "**Sloupce:** id_stanice, mesic, den, hodina, teplota, flag, latitude, longitude, vyska, stat, nazev\n",
    "\n",
    "*Poznámka: Spark umí v metodě textFile načíst celý adresář (spojí všechny soubory v adresáři do jednoho RDD).*\n",
    "\n",
    "#### Zadání\n",
    "\n",
    "4.1. Prohlédněte si několik prvních řádků z některého souboru, abyste si připomněli, co v datech je.  \n",
    "4.2. Zjistěte, který stát ma nejvyšší průměrnou teplotu z měření jen za měsíce 6&ndash;8. Teplotu uveďte ve stupních Celsia.  \n",
    "4.3. Pro každý měsíc vypište stát, který má nejvyšší průměrnou teplotu z měření za tento měsíc.\n",
    "\n",
    "\n",
    "#### Očekávaný výstup\n",
    "\n",
    "V zadání 4.2\n",
    "\n",
    "| stat | prum_teplota |  \n",
    "| ---- | ------------:|  \n",
    "\n",
    "V zadání 4.3\n",
    "\n",
    "| mesic | stat | prum_teplota |  \n",
    "| ----- | ---- | ------------:|  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nacteni RDD, odfiltrovani nezadoucich radku\n",
    "# !!! zde je treba dosadit cestu do vlastniho user adresare na HDFS\n",
    "lines = sc.textFile(\"/user/pascepet/data/teplota\")\n",
    "\n",
    "# vyhozeni radku s nazvy sloupcu a ponechani ostatnich\n",
    "lines2 = lines.filter(lambda line: not(re.match(r'stanice', line)))\n",
    "\n",
    "# prevod radku na strukturu ((mesic, stat), teplota):\n",
    "# rozdeleni radku\n",
    "# ponechani jen radku, kde paty prvek (teplota) je neprazdny\n",
    "# konverze z Fahrenheita na Celsia\n",
    "recs = lines2.map(lambda line: line.split(\",\")) \\\n",
    "    .filter(lambda rec: rec[4]!='') \\\n",
    "    .map(lambda rec: ((int(rec[1]), rec[9]), (float(rec[4])/10-32)*5/9))\n",
    "\n",
    "# nakesovani\n",
    "from pyspark import StorageLevel\n",
    "recs.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# stat s nejvetsi prumernou teplotou za mesice 6-8\n",
    "# vyhodit udaje za jine mesice\n",
    "recs1 = recs.filter(lambda rec: rec[0][0] in range(6,9))\n",
    "# ponecha jen nazev statu (mesic nas uz nezajima), k tomu pripravi dvojice pro agregaci\n",
    "recs1 = recs1.map(lambda rec: (rec[0][1], (1, rec[1])))\n",
    "result1 = recs1.reduceByKey(lambda s,t: (s[0]+t[0], s[1]+t[1]))  # agregace po statech - pocet a soucet \n",
    "result1 = result1.map(lambda res: (res[0], res[1][1]/res[1][0])) # prumer = soucet/pocet\n",
    "result1 = result1.sortBy(lambda res: res[1], False)\n",
    "result1.take(1)\n",
    "\n",
    "# za kazdy mesic stat s nejvetsi prumernou teplotou\n",
    "# spocita se agregace za kazdou dvojici (mesic, stat)\n",
    "recs2 = recs.map(lambda rec: (rec[0], (1, rec[1])))\n",
    "result2 = recs2.reduceByKey(lambda s,t: (s[0]+t[0], s[1]+t[1]))\n",
    "result2 = result2.map(lambda res: (res[0], res[1][1]/res[1][0])) # nyni mame za kazdou dvojici (mesic, stat) prumernou teplotu\n",
    "result2 = result2.map(lambda res: (res[0][0], (res[0][1], res[1]))) # dame mesic jako klic a mezi staty budeme hledat maximum \n",
    "result3 = result2.reduceByKey(lambda s,t: (t if t[1]>s[1] else s))\n",
    "result3 = result3.sortBy(lambda res: res[0], True)\n",
    "result3.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
