{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL in simple examples\n",
    "\n",
    "This section is used to introduce the basic syntax of Spark SQL operations using simple sample examples. For more on each method, see [Manual Spark SQL](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How to start Pyspark\n",
    "\n",
    "`export PYSPARK_PYTHON=python3`  \n",
    "`pyspark --master yarn --num-executors 2 --executor-memory 4G --conf spark.ui.port=1<ddmm>`, kde `<ddmm>` je váš den a měsíc narození, např. `spark.ui.port=10811`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpfull import\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataFrame load from databaze Hive and cache\n",
    "Tep_DF = spark.sql('select * from fel_bigdata.trips').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tep_DF.show() # print to console as  a DataFrame\n",
    "Tep_DF.take(5) # print to console as  RDD\n",
    "\n",
    "Tep_DF.count() # number of rows\n",
    "\n",
    "Tep_DF.printSchema() # print schema - columns names and types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns and rows selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### columns selection\n",
    "Tep_DF2 = Tep_DF.select('route_id', 'service_id', 'bikes_allowed')\n",
    "Tep_DF2.show()\n",
    "\n",
    "### rows selection (different types of condition)\n",
    "Tep_DF2 = Tep_DF.filter(Tep_DF['bikes_allowed']=='1')\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.filter((Tep_DF['bikes_allowed']=='1') & (Tep_DF['route_id']=='L991')) # brackets are necessary\n",
    "Tep_DF2.show()\n",
    "\n",
    "\n",
    "Tep_DF2 = Tep_DF.filter('bikes_allowed=\"1\"')\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.filter('bikes_allowed=\"1\" and route_id=\"L991\"')\n",
    "Tep_DF2.show()\n",
    "\n",
    "# RDD syntax\n",
    "Tep_DF2.rdd.filter(lambda r: r[3]=='1' and r[1]=='L991').take(5)  \n",
    "\n",
    "### unique rows ( no diplicities)\n",
    "Tep_DF2 = Tep_DF.select('route_id', 'service_id', 'bikes_allowed').distinct()\n",
    "Tep_DF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column\n",
    "Tep_DF2 = Tep_DF.withColumn('bikes_allowed_1', Tep_DF['bikes_allowed']+100)\n",
    "Tep_DF2.show()\n",
    "\n",
    "# column value rewrite\n",
    "Tep_DF2 = Tep_DF.withColumn('bikes_allowed',Tep_DF['bikes_allowed']+100)\n",
    "Tep_DF2.show()\n",
    "\n",
    "# If you need to  to apply spark Function you have firstly import a specific modul\n",
    "# see http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "# from pyspark.sql import functions as F\n",
    "Tep_DF2 = Tep_DF.withColumn('trip_name', F.lower(Tep_DF['trip_short_name']))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('trip_short_name_split', F.size(F.split(Tep_DF['trip_short_name'], \" \")))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('route_id2', F.regexp_replace(Tep_DF['route_id'], 'L', 'Linka'))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('bikes_yes_no', F.when(Tep_DF['bikes_allowed']=='1', 'yes').otherwise('no'))\n",
    "Tep_DF2.show()\n",
    "\n",
    "### work with missing values\n",
    "Tep_DF2 = Tep_DF.dropna() # drop rows with null values\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.fillna(0, 'exceptional')\n",
    "\n",
    "### Column rename\n",
    "# one column\n",
    "Tep_DF2 = Tep_DF.withColumnRenamed('bikes_allowed', 'are_bikes_allowed')\n",
    "Tep_DF2.show()\n",
    "\n",
    "# all column in  DataFrame\n",
    "Tep_DF2 = Tep_DF.select('route_id', 'service_id', 'bikes_allowed').toDF('linka', 'route_uniq_id', 'are_bikes_allowed')\n",
    "Tep_DF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sorting\n",
    "Tep_DF2 = Tep_DF.orderBy('route_id', ascending=False)\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.orderBy(Tep_DF['route_id'].desc())\n",
    "Tep_DF2.show()\n",
    "\n",
    "### agg\n",
    "Tep_DF2 = Tep_DF.groupBy('route_id').count()\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.groupBy('mesic').agg({'teplota': 'avg'}) # alternatively\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.groupBy('stat').count()\n",
    "Tep_DF2.show()\n",
    "\n",
    "# agregation accros whole DataFrame\n",
    "Tep_DF2 = Tep_DF.max('teplota') # won't work, firstly you have to groupBy operation\n",
    "Tep_DF2 = Tep_DF.groupBy().max('teplota') # now is working\n",
    "Tep_DF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join - joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second table definition as dataFrame\n",
    "Lines = spark.createDataFrame([('L991', 'Linka 991'), ('L332', 'Linka 332'), ('L333', 'Linka 333')], ('route_id', 'route_name'))\n",
    "Lines.show()\n",
    "\n",
    "# join using same columns names in both dataFrames\n",
    "Tep_DF2 = Tep_DF.join(Lines, 'route_id')\n",
    "Tep_DF2.show()\n",
    "\n",
    "# join with condition - all columns from both dataFrame are included\n",
    "Tep_DF2 = Tep_DF.join(Lines, Tep_DF['route_id']==Lines['route_id'])\n",
    "Tep_DF2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ce3b0a56a865d480673a9187eace210a751ef3304bdc5d2817c20bdd65e6d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
