{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL v jednoduchých příkladech\n",
    "\n",
    "Tato část slouží k seznámení se základní syntaxí operací Spark SQL pomocí jednoduchých ukázkových příkladů. Více k jednotlivým metodám viz [Manual Spark SQL](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spuštění PySpark\n",
    "\n",
    "`export PYSPARK_PYTHON=python3`  \n",
    "`pyspark --master yarn --num-executors 2 --executor-memory 4G --conf spark.ui.port=1<ddmm>`, kde `<ddmm>` je váš den a měsíc narození, např. `spark.ui.port=10811`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uzitecny import\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nacteni DataFrame z databaze Hive a nakesovani\n",
    "Tep_DF = spark.sql('select * from fel_bigdata.teplota').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Základní informace o DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tep_DF.show() # vypis ukazky dat jako DataFrame\n",
    "Tep_DF.take(5) # vypis ukazky dat jako RDD\n",
    "\n",
    "Tep_DF.count() # pocet radku\n",
    "\n",
    "Tep_DF.printSchema() # schema - nazvy sloupcu a typy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Výběr sloupců a řádků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vyber sloupcu\n",
    "Tep_DF2 = Tep_DF.select('stat', 'mesic', 'teplota')\n",
    "Tep_DF2.show()\n",
    "\n",
    "### vyber radku - ruzne zpusoby\n",
    "Tep_DF2 = Tep_DF.filter(Tep_DF['stat']=='TX')\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.filter((Tep_DF['stat']=='TX') & (Tep_DF['mesic']==11)) # zavorky jsou nutne\n",
    "Tep_DF2.show()\n",
    "\n",
    "# to same, ale podminka v jine syntaxi\n",
    "Tep_DF2 = Tep_DF.filter('stat=\"TX\"')\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.filter('stat=\"TX\" and mesic=11')\n",
    "Tep_DF2.show()\n",
    "\n",
    "# RDD syntaxe\n",
    "Tep_DF.filter(lambda r: r[9]=='TX' and r[1]==11).take(5) # nefunguje, ocekava syntaxi pro DataFrame  \n",
    "Tep_DF.rdd.filter(lambda r: r[9]=='TX' and r[1]==11).take(5) # po explicitnim prevodu na RDD funguje \n",
    "\n",
    "### pouze radky bez duplicit (unikatni hodnoty)\n",
    "Tep_DF2 = Tep_DF.select('stanice', 'stat', 'nazev').distinct()\n",
    "Tep_DF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformace sloupců"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vytvoreni noveho sloupce\n",
    "Tep_DF2 = Tep_DF.withColumn('teplota_f', Tep_DF['teplota']*9.0/5.0 + 32)\n",
    "Tep_DF2.show()\n",
    "\n",
    "# prepsani existujiciho sloupce\n",
    "Tep_DF2 = Tep_DF.withColumn('teplota', Tep_DF['teplota']*9.0/5.0 + 32)\n",
    "Tep_DF2.show()\n",
    "\n",
    "# pokud potrebuji aplikovat funkci na kazdy prvek sloupce: Spark SQL sloupcove (vektorove) funkce\n",
    "# viz http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "# je potreba importovat modul\n",
    "# from pyspark.sql import functions as F\n",
    "Tep_DF2 = Tep_DF.withColumn('stanice2', F.lower(Tep_DF['stanice']))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('nazev_delka', F.length(Tep_DF['nazev']))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('nazev_slov', F.size(F.split(Tep_DF['nazev'], \" \")))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('nazev2', F.regexp_replace(Tep_DF['nazev'], 'A', '4'))\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.withColumn('den2', F.when(Tep_DF['den']<=10, '1-10').otherwise('11-31'))\n",
    "Tep_DF2.show()\n",
    "\n",
    "### prace s chybejicimi hodnotami\n",
    "Tep_DF2 = Tep_DF.dropna() # vyhodi radky s aspon jednou hodnotou null\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.fillna(0, 'teplota')\n",
    "Tep_DF2.show()\n",
    "\n",
    "### prejmenovani sloupcu\n",
    "# jeden sloupec\n",
    "Tep_DF2 = Tep_DF.withColumnRenamed('latitude', 'gps_lat')\n",
    "Tep_DF2.show()\n",
    "\n",
    "# vsechny sloupce v DataFrame\n",
    "Tep_DF2 = Tep_DF.select('stat', 'latitude', 'longitude').toDF('usa_state', 'gps_lat', 'gps_long')\n",
    "Tep_DF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Řazení, agregace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### razeni\n",
    "Tep_DF2 = Tep_DF.orderBy('latitude', ascending=False)\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.orderBy(Tep_DF['latitude'].desc())\n",
    "Tep_DF2.show()\n",
    "\n",
    "### agregace\n",
    "Tep_DF2 = Tep_DF.groupBy('mesic').avg('teplota')\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.groupBy('mesic').agg({'teplota': 'avg'}) # to same v obecnejsim zapisu\n",
    "Tep_DF2.show()\n",
    "\n",
    "Tep_DF2 = Tep_DF.groupBy('stat').count()\n",
    "Tep_DF2.show()\n",
    "\n",
    "# agregace pres cely DataFrame\n",
    "Tep_DF2 = Tep_DF.max('teplota') # nebude fungovat, je treba udelat prazdny groupBy\n",
    "Tep_DF2 = Tep_DF.groupBy().max('teplota') # takto funguje, vysledek je DataFrame\n",
    "Tep_DF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join - spojování tabulek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# druha tabulka - na pripojeni k vychozimu DataFrame\n",
    "States = spark.createDataFrame([('CA', 'California'), ('TX', 'Texas'), ('KY', 'Kentucky')], ('stat', 'stat_nazev'))\n",
    "States.show()\n",
    "\n",
    "# join pri shode jmen pole, pres ktere se joinuje\n",
    "Tep_DF2 = Tep_DF.join(States, 'stat')\n",
    "Tep_DF2.show()\n",
    "\n",
    "# join s obecnejsi joinovaci podminkou - bude obsahovat joinovaci pole 2x\n",
    "Tep_DF2 = Tep_DF.join(States, Tep_DF['stat']==States['stat'])\n",
    "Tep_DF2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
