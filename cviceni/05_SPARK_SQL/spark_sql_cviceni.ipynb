{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spuštění PySpark\n",
    "\n",
    "`export PYSPARK_PYTHON=python3`\n",
    "\n",
    "`pyspark --master yarn --num-executors 2 --executor-memory 4G --conf spark.ui.port=1<ddmm>`, kde `<ddmm>` je váš den a měsíc narození, např. `spark.ui.port=10811`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful import\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# you can use hive_02/PID_GTFS/...\n",
    "df = spark.read.csv('/your_user/spark_05/PID_GTFS/trips.txt', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Cache the file in memory.\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 Write out a sample of the data.\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 Write out a basic exploration of the data.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.5 Find out the total number of records (rows) in the DataFrame. (72 530)\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 Find out how many unique routes are there. (816)\n",
    "\n",
    "df_2 = df.select('route_id').distinct()\n",
    "df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.7 Find out the lowest and highest route number are there. (1, 3360)\n",
    "\n",
    "df_2 = df.withColumn('route_id_number', F.split('route_id', 'L').getItem(1).cast('integer'))\n",
    "df_2.agg(F.min(F.col('route_id_number')), F.max(F.col('route_id_number'))).show()\n",
    "\n",
    "+--------------------+--------------------+\n",
    "|min(route_id_number)|max(route_id_number)|\n",
    "+--------------------+--------------------+\n",
    "|                   1|                3360|\n",
    "+--------------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.8 Find out the number of trips for both direction for route L1. (331,313)\n",
    "\n",
    "df_l1=df.filter(df['route_id'] == 'L1')\n",
    "df_l1.groupBy('direction_id').count().show()\n",
    "+------------+-----+\n",
    "|direction_id|count|\n",
    "+------------+-----+\n",
    "|           0|  331|\n",
    "|           1|  313|\n",
    "+------------+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.9 Find out a count of night trips. (1258)\n",
    "\n",
    "df_routes=spark.read.csv('data/trips/routes.txt', header=True)\n",
    "df_all = df.join(df_routes, df_routes['route_id']==df['route_id'])\n",
    "df_all.filter(df_all['is_night'] == '1').count()\n",
    "\n",
    "1258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.10 Additionally: Create a temporary table from the DataFrame and try to do 1.5–1.8 using SQL.\n",
    "Example:\n",
    "df.createOrReplaceTempView('Trips')\n",
    "spark.sql('select * from Trips limit 10').show()\n",
    "\n",
    "1. 5. using SQL and TempView\n",
    "spark.sql('select count(route_id) from Trips').show()\n",
    "+---------------+\n",
    "|count(route_id)|\n",
    "+---------------+\n",
    "|          93580|\n",
    "+---------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2 Exclude all records that have a year listed outside the 1950--2018 range. Determine how many records remain in the DataFrame. (362 221)\n",
    "\n",
    "songs_df = spark.read\\\n",
    " .format('csv')\\\n",
    " .option('header', 'false')\\\n",
    " .option('delimiter', ',')\\\n",
    " .option('inferSchema', 'true')\\\n",
    " .load('data/lyrics.csv')\n",
    "\n",
    "# Rename columns to the ones that make sense\n",
    "songs_df = songs_df.withColumnRenamed('_c0', 'id')\\\n",
    " .withColumnRenamed('_c1', 'name')\\\n",
    " .withColumnRenamed('_c2', 'year')\\\n",
    " .withColumnRenamed('_c3', 'singer')\\\n",
    " .withColumnRenamed('_c4', 'genre')\\\n",
    " .withColumnRenamed('_c5', 'text')\n",
    "\n",
    "\n",
    "songs_df2 = songs_df.filter('year >= 1950 and year <= 2018')\n",
    "songs_df2.cache()\n",
    "songs_df2.count()\n",
    "\n",
    "362221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 Edit the lyrics\n",
    "\n",
    "# Replace missing values in the lyrics column with empty strings.\n",
    "songs_df2 = songs_df2.fillna('', 'text')\n",
    "\n",
    "# Convert the text to lower case.\n",
    "songs_df2 = songs_df2.withColumn('text', F.lower(songs_df2['text']))\n",
    "\n",
    "# Replace all non-alphanumeric characters with a space.\n",
    "songs_df2 = songs_df2.withColumn('text', F.regexp_replace(songs_df2['text'], '[\\W ]', ' '))\n",
    "\n",
    "# Replace multiple space sequences with a single space.\n",
    "songs_df2 = songs_df2.withColumn('text', F.regexp_replace(songs_df2['text'], '[ ]+', ' '))\n",
    "\n",
    "# Omit spaces on both edges of the text (trim function).\n",
    "songs_df2 = songs_df2.withColumn('text', F.trim(songs_df2['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 Add a words_poc column to the DataFrame containing the number of all words in the song.\n",
    "\n",
    "songs_df2 = songs_df2.withColumn('slova_poc', F.size(F.split(songs_df2['text'], ' ')))\n",
    "songs_df2.filter('text=\"\"').show()\n",
    "\n",
    "songs_df2 = songs_df2.withColumn('slova_poc', F.when(songs_df2['text']=='', 0).otherwise(songs_df2['slova_poc']))\n",
    "songs_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4  Cache the resulting DataFrame again.\n",
    "\n",
    "songs_df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 Find out how many artists have at least 500 songs and who they are. Create a separate DataFrame for these artists, use it in Assignment 4.3. (19; Bob Dylan 614, Chris Brown 655, etc.)\n",
    "\n",
    "\n",
    "singers = songs_df2.groupBy('singer').count().toDF('singer', 'pocet').filter(\"pocet >= 500\")\n",
    "singers.show()\n",
    "singers.count()\n",
    "19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Considering only songs with non-empty lyrics (i.e., word counts greater than 0), which artist with at least 100 such songs has the highest average number of words per song? (eightball-mjg 627.9)\n",
    "\n",
    "songs_df2.filter('slova_poc > 0') \\\n",
    "    .groupBy('singer').agg({'*':'count', 'slova_poc':'avg'}) \\\n",
    "    .toDF('singer', 'prumer', 'pocet').filter('pocet>=100') \\\n",
    "    .orderBy('prumer', ascending=False) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1  Find the 20 most frequently occurring words of at least two characters in song lyrics. (Count each word as many times as it appears in the text. Here it is useful to process the DataFrame using RDD transformations.)\n",
    "\n",
    "words_top = songs_df2.rdd \\\n",
    "     .flatMap(lambda r: r[5].split(\" \")) \\\n",
    "     .filter(lambda r: len(r)>1) \\\n",
    "     .map(lambda r: (r, 1)) \\\n",
    "     .reduceByKey(lambda a,b: a+b) \\\n",
    "     .sortBy(lambda r: r[1], False)\n",
    "    \n",
    "words_top.take(20)\n",
    "\n",
    "[(u'the', 2031323), (u'you', 1988108), (u'to', 1181198), \n",
    "(u'and', 1153519), (u'it', 910863), (u'me', 859221), \n",
    "(u'my', 717654), (u'in', 685757), (u'of', 567866), \n",
    "(u'that', 555589), (u'your', 499245), (u'on', 467524), \n",
    "(u'we', 454191), (u'all', 405964), (u'is', 389578), \n",
    "(u'for', 368171), (u'be', 363155), (u'can', 354989), \n",
    "(u'so', 324009), (u'no', 313001)]\n",
    "\n",
    "\n",
    "stopw = sc.textFile(\"/your_user/spark_05/stopwords.txt\").collect()\n",
    "stopw = set(stopw)\n",
    "\n",
    "words_top2 = songs_df2.rdd \\\n",
    "    .flatMap(lambda r: r[5].split(\" \")) \\\n",
    "    .filter(lambda r: len(r)>1) \\\n",
    "    .filter(lambda r: r not in stopw) \\\n",
    "    .map(lambda r: (r, 1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .sortBy(lambda r: r[1], False)\n",
    "    \n",
    "words_top2.take(20)\n",
    "\n",
    "[(u'love', 310137), (u'don', 308914), (u'll', 222791), \n",
    "(u'time', 174700), (u've', 164238), (u'baby', 141879), \n",
    "(u'yeah', 131658), (u'life', 113901), (u'feel', 104905), \n",
    "(u'la', 103182), (u'gonna', 96730), (u'heart', 95538), \n",
    "(u'day', 90366), (u'night', 89188), (u'man', 88579), \n",
    "(u'ain', 86083), (u'wanna', 80072), (u'girl', 79097), \n",
    "(u'de', 77752), (u'good', 70872)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Choose three of your choice from the set of most frequent non-stop-words. Add three columns to the DataFrame (one column for each word) with a 1/0 flag to indicate whether the word is mentioned at least once in the song.\n",
    "\n",
    "songs_df2 = songs_df2.withColumn('is_love', F.when(F.regexp_extract(songs_df2['text'], r'\\b(love)\\b', 1) == 'love', 1).otherwise(0))\n",
    "songs_df2 = songs_df2.withColumn('is_like', F.when(F.regexp_extract(songs_df2['text'], r'\\b(like)\\b', 1) == 'like', 1).otherwise(0))\n",
    "songs_df2 = songs_df2.withColumn('is_know', F.when(F.regexp_extract(songs_df2['text'], r'\\b(know)\\b', 1) == 'know', 1).otherwise(0))\n",
    "songs_df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3  For performers with at least 500 songs (see 3.1), find out what proportion of their songs contain the three common words you selected from Assignment 4.2.\n",
    "\n",
    "singers_words = singers.join(songs_df2, 'singer') \\\n",
    "    .select('singer', 'is_love', 'is_like', 'is_know') \\\n",
    "    .groupBy('singer') \\\n",
    "    .agg({'is_love':'avg', 'is_like':'avg', 'is_know':'avg'})\n",
    "singers_words.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ce3b0a56a865d480673a9187eace210a751ef3304bdc5d2817c20bdd65e6d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
